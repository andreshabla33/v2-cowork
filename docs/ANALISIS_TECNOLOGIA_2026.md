# An√°lisis de Tecnolog√≠as de Visi√≥n Artificial en Navegador (Tendencias 2026)

**Fecha:** 2026-01-29
**Objetivo:** Evaluar viabilidad de mantener MediaPipe vs. pivotar a nuevas tecnolog√≠as Open Source.

## Resumen Ejecutivo

Para el caso de uso de **an√°lisis biom√©trico en tiempo real (facial y postural) en videollamadas**, **MediaPipe sigue siendo la opci√≥n l√≠der en rendimiento y eficiencia** para 2025-2026. Sin embargo, el ecosistema se est√° moviendo hacia **WebGPU** y runtimes universales como **ONNX Runtime Web** y **Transformers.js**.

**Recomendaci√≥n:** ‚úÖ **Mantener MediaPipe** a corto/mediano plazo, pero migrar la implementaci√≥n a **WebGPU** (ya soportado experimentalmente en MediaPipe) y encapsularlo en Workers (como estamos haciendo). Pivotar a Transformers.js solo si se requieren modelos de lenguaje/visi√≥n m√°s complejos (ej. descripciones sem√°nticas de video) que detecci√≥n de puntos.

---

## 1. Panorama Tecnol√≥gico 2025-2026

### A. Google MediaPipe (Tecnolog√≠a Actual)
*   **Estado:** Est√°ndar de facto para detecci√≥n de landmarks en web.
*   **Ventajas:**
    *   Modelos extremadamente ligeros (<10MB) y r√°pidos.
    *   Optimizado espec√≠ficamente para Face Mesh y Pose (BlazeFace/BlazePose).
    *   Soporte WebAssembly (WASM) y WebGL/WebGPU.
*   **Desventajas:**
    *   Menos flexible para modelos "custom" fuera del ecosistema TFLite.
    *   Caja negra en el pre-procesamiento.

### B. Transformers.js (Hugging Face) + ONNX Runtime Web
*   **Estado:** La tendencia de mayor crecimiento. Permite correr modelos de Hugging Face en el navegador.
*   **Ventajas:**
    *   **Acceso a SOTA:** Puedes usar cualquier modelo moderno (YOLOv11, ViT, Depth-Anything) convertido a ONNX.
    *   **Agn√≥stico:** No depende de Google. Open Source puro.
    *   **WebGPU First:** Dise√±ado para aprovechar aceleraci√≥n de hardware moderna.
*   **Desventajas:**
    *   **Peso:** Los modelos suelen ser m√°s grandes que los micro-modelos de MediaPipe.
    *   **Overhead:** Mayor consumo de memoria inicial.

### C. TensorFlow.js (Legacy)
*   **Estado:** En mantenimiento. Google prioriza MediaPipe para soluciones "listas para usar" y TFLite para edge.
*   **Recomendaci√≥n:** No migrar aqu√≠. Es tecnolog√≠a 2020-2023.

---

## 2. Comparativa para "V2 Cowork"

| Caracter√≠stica | MediaPipe (Actual) | Transformers.js / ONNX |
| :--- | :--- | :--- |
| **Detecci√≥n Facial** | üöÄ **Excelente** (468 puntos, <5ms) | ‚ö†Ô∏è Bueno, pero modelos m√°s pesados |
| **Detecci√≥n Pose** | üöÄ **Excelente** (33 puntos, ligero) | ‚úÖ Muy bueno (YOLOv8-Pose), pero m√°s lento |
| **Carga Inicial** | ‚ö° **R√°pida** (WASM modular) | üê¢ Lenta (Modelos ONNX >20MB) |
| **Consumo CPU/GPU** | üü¢ **Bajo** (Optimizado Edge) | üü° Medio (Depende del modelo) |
| **Flexibilidad** | üî¥ Baja (Solo tareas predefinidas) | üü¢ Alta (Cualquier modelo) |

## 3. Hoja de Ruta Tecnol√≥gica (Roadmap)

### Fase 1: Optimizaci√≥n Actual (Q1 2026)
*   **Web Workers:** Desacoplar procesamiento del hilo principal (‚úÖ Implementado).
*   **WebGPU Delegate:** Activar `delegate: 'GPU'` en MediaPipe en lugar de WebGL para reducir uso de CPU.

### Fase 2: Evaluaci√≥n de Pivote (Q3 2026)
*   Si requerimos **an√°lisis de emociones m√°s complejo** (no solo geometr√≠a facial, sino contexto visual), evaluar **Transformers.js** con modelos peque√±os de Vision-Language (ej. Moondream optimizado para web).

## Conclusi√≥n T√©cnica

El problema actual ("audio entrecortado" o fallos de carga) **no es culpa de la tecnolog√≠a MediaPipe**, sino de la **arquitectura de implementaci√≥n (Hilo principal vs Worker)** y de la **integraci√≥n con bundlers modernos (Vite)**.

**Cambiar de tecnolog√≠a ahora introducir√≠a:**
1.  Mayor latencia de descarga (modelos m√°s pesados).
2.  Mayor complejidad de ingenier√≠a.
3.  P√©rdida de las optimizaciones espec√≠ficas de BlazeFace/BlazePose.

**Decisi√≥n:** Reparar la integraci√≥n del Worker de MediaPipe es el camino correcto.
